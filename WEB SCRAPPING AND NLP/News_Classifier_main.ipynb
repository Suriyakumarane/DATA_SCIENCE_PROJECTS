{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9Xa607UVY8"
      },
      "source": [
        "# **NEWS SCRAPPING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aqjS7ZukUW0_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to 'bbc_news.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from urllib.parse import urljoin  # For joining URLs\n",
        "\n",
        "def scrape_bbc_news():\n",
        "    base_url = 'https://www.bbc.com'\n",
        "    url = 'https://www.bbc.com/news'  # URL of the BBC News website\n",
        "    data = []\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find sections/categories on the BBC News website\n",
        "        sections = soup.find_all('a', class_='nw-o-link')\n",
        "\n",
        "        for section in sections:\n",
        "            section_name = section.text.strip()\n",
        "            section_link = urljoin(base_url, section['href'])  # Join base URL with section link\n",
        "\n",
        "            print(f\"Scraping articles from '{section_name}' section...\")\n",
        "            section_data = scrape_section(section_link)\n",
        "            data.extend(section_data)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        # Save data to CSV file\n",
        "        save_to_csv(data)\n",
        "\n",
        "    else:\n",
        "        print('Failed to fetch data from BBC News')\n",
        "\n",
        "def scrape_section(section_url):\n",
        "    section_data = []\n",
        "    # Send a GET request to the section URL\n",
        "    response = requests.get(section_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all article elements\n",
        "        articles = soup.find_all('div', class_='gs-c-promo')\n",
        "\n",
        "        # Extract titles and content of each article\n",
        "        for article in articles:\n",
        "            # Extract title\n",
        "            title_element = article.find('h3', class_='gs-c-promo-heading__title')\n",
        "            title = title_element.text.strip() if title_element else 'No title'\n",
        "\n",
        "            # Extract content (if available)\n",
        "            content_element = article.find('p', class_='gs-c-promo-summary')\n",
        "            content = content_element.text.strip() if content_element else 'No content available'\n",
        "\n",
        "            section_data.append({'Title': title, 'Content': content})\n",
        "\n",
        "    else:\n",
        "        print(f'Failed to fetch data from {section_url}')\n",
        "\n",
        "    return section_data\n",
        "\n",
        "def save_to_csv(data):\n",
        "    # Define CSV file name\n",
        "    file_name = 'bbc_news.csv'\n",
        "\n",
        "    # Write data to CSV file\n",
        "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Title', 'Content']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Write header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write rows\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Data saved to '{file_name}'\")\n",
        "\n",
        "# Call the function to initiate scraping and save data to CSV\n",
        "scrape_bbc_news()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
